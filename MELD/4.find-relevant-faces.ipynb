{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitdevpython378e162af75d134820b03d49898b79756f",
   "display_name": "Python 3.7.9 64-bit ('dev-python-3.7')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9988\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "VISUAL_FEATURES_PATH = '/home/tk/datasets/MELD/visual-features/MELD-visual-features/train/'\n",
    "VIDS_DIR = \"/home/tk/datasets/MELD/MELD.Raw/train/train_splits/\"\n",
    "DATASET_PATH = '/home/tk/datasets/MELD/visual-features/MELD-visual-features/datasets.json'\n",
    "\n",
    "with open(DATASET_PATH, 'r') as stream:\n",
    "    datasets = json.load(stream)\n",
    "\n",
    "visual_features = glob(os.path.join(VISUAL_FEATURES_PATH, '*.npy'))\n",
    "visual_features = {os.path.basename(vf).split('.npy')[0] : np.load(vf, allow_pickle=True).item() for vf in visual_features}\n",
    "\n",
    "print(len(visual_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9989.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cfbc3b7eb7d4b999d9c78d4fb068ad4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'dia125_utt3' doesn't exist\n",
      "\n",
      "Out of the 9989 number of videos (utterances),\n",
      "There are in total of 6 unique speakers mentioned\n",
      "\n",
      "['Chandler', 'Joey', 'Monica', 'Phoebe', 'Rachel', 'Ross']\n",
      "\n",
      "and 71883 faces detected\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "FACE_PROB = 0.975\n",
    "EVERY_N_FRAME = 16\n",
    "ACTORS = ['Chandler', 'Joey', 'Monica', 'Phoebe', 'Rachel', 'Ross']\n",
    "DATASET_chosen = 'train'\n",
    "dataset_chosen = datasets[DATASET_chosen]\n",
    "\n",
    "speakers_mentioned = []\n",
    "embeddings_all = []\n",
    "\n",
    "# This is gonna help us to find back to the source frame and video\n",
    "idx2source = {}\n",
    "embeddings_all = []\n",
    "bboxes_all = []\n",
    "landmarks_all = []\n",
    "\n",
    "count = 0\n",
    "for diautt, annot in tqdm(dataset_chosen.items()):\n",
    "    # There is one face annotated in the entire video.\n",
    "    # We are not even sure if the face is actually there or not.\n",
    "    # Even though the face is there, we are not sure which frame number it is.\n",
    "    try:\n",
    "        if annot['Speaker'] not in ACTORS:\n",
    "            continue\n",
    "\n",
    "        for framenum, list_of_findings in visual_features[diautt].items():\n",
    "            if framenum % EVERY_N_FRAME != 0:\n",
    "                continue\n",
    "            for finding in list_of_findings:\n",
    "                if finding['bbox'][-1] < FACE_PROB:\n",
    "                    continue\n",
    "                \n",
    "                embeddings_all.append(finding['embedding'])\n",
    "                bboxes_all.append(finding['bbox'])\n",
    "                landmarks_all.append(finding['landmark'])\n",
    "                idx2source[count] = {'diautt':diautt, 'frame': framenum}\n",
    "                count+=1\n",
    "                speakers_mentioned.append(annot['Speaker'])\n",
    "    except KeyError as e:\n",
    "        print(f\"{e} doesn't exist\")\n",
    "        continue\n",
    "\n",
    "assert len(embeddings_all) == len(bboxes_all) == len(landmarks_all) == \\\n",
    "        len(idx2source)\n",
    "\n",
    "speakers_mentioned = sorted(list(set(speakers_mentioned)))\n",
    "\n",
    "print(f\"Out of the {len(dataset_chosen)} number of videos (utterances),\")\n",
    "print(f\"There are in total of {len(speakers_mentioned)} unique speakers mentioned\")\n",
    "print()\n",
    "print(speakers_mentioned)\n",
    "print()\n",
    "print(f\"and {len(embeddings_all)} faces detected\")\n",
    "\n",
    "np.save('./DEBUG/embeddings-all.npy', embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 28721\n",
      "Silhouette Coefficient: 0.122\n",
      "Number of faces that are clustered: 43162\n",
      "\n",
      " label -1 \t has 28721 counts\n",
      " label 0 \t has 6833 counts\n",
      " label 1 \t has 2471 counts\n",
      " label 2 \t has 7505 counts\n",
      " label 3 \t has 6275 counts\n",
      " label 4 \t has 5144 counts\n",
      " label 5 \t has 6134 counts\n",
      " label 6 \t has 8548 counts\n",
      " label 7 \t has 252 counts\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X = np.stack(embeddings_all)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "# DBSCAN uses euclidean distance between the data points.\n",
    "# TODO: find a way to replace it with angle distance.\n",
    "# eps and min_samples are hyper parameters that you have to tune.\n",
    "# At the moment 0.75 and 10, respectively, works decent.\n",
    "db = DBSCAN(eps=0.8, min_samples=100, n_jobs=-1).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "print(f\"Number of faces that are clustered: {len(embeddings_all) - n_noise_}\")\n",
    "print()\n",
    "\n",
    "(label_num, counts) = np.unique(labels, return_counts=True)\n",
    "\n",
    "for l, c in zip(label_num, counts):\n",
    "    print(f\" label {l} \\t has {c} counts\")\n",
    "\n",
    "np.save('./DEBUG/embeddings-clusters.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=71883.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54a303f7122445ccae12df0dbbccb5c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=71883.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d7ec10b6bd644d29500fd80f70bb2ee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9d66a21dc5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mframe_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mav/video/frame.pyx\u001b[0m in \u001b[0;36mav.video.frame.VideoFrame.to_image\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dev-python-3.7/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dev-python-3.7/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import av\n",
    "import random\n",
    "from cltl_face_all.face_alignment import FaceDetection\n",
    "\n",
    "labels = np.load(\"./DEBUG/embeddings-clusters.npy\")\n",
    "\n",
    "shutil.rmtree('./DEBUG/faces/', ignore_errors=True)\n",
    "\n",
    "assert len(embeddings_all) == len(bboxes_all) == len(landmarks_all) == \\\n",
    "        len(idx2source) == len(labels)\n",
    "\n",
    "\n",
    "list_all = []\n",
    "\n",
    "\n",
    "indices = list(idx2source.keys())\n",
    "for idx in tqdm(indices):\n",
    "    label_ = labels[idx]\n",
    "\n",
    "    embedding_ = embeddings_all[idx]\n",
    "    bbox_ = bboxes_all[idx]\n",
    "    landmark_ = landmarks_all[idx]\n",
    "    source_ = idx2source[idx]\n",
    "\n",
    "    to_append = {'label': label_, \n",
    "                'embedding': embedding_,\n",
    "                 'bbox': bbox_,\n",
    "                 'landmark': landmark_,\n",
    "                 'diautt': source_['diautt'],\n",
    "                 'frame': source_['frame']}\n",
    "\n",
    "    list_all.append(to_append)\n",
    "\n",
    "\n",
    "assert len(list_all) == len(labels)\n",
    "\n",
    "random.shuffle(list_all)\n",
    "\n",
    "fd = FaceDetection(device='cpu', face_detector='sfd')\n",
    "\n",
    "\n",
    "\n",
    "for finding in tqdm(list_all):\n",
    "    label_ = finding['label']\n",
    "    embedding_ = finding['embedding']\n",
    "    bbox_ = finding['bbox']\n",
    "    landmark_ = finding['landmark']\n",
    "    diautt_ = finding['diautt']\n",
    "    frame_num = finding['frame']\n",
    "    video_path = os.path.join(VIDS_DIR, diautt_) + '.mp4'\n",
    "\n",
    "    os.makedirs(os.path.join('./DEBUG/faces', str(label_)), exist_ok=True)\n",
    "\n",
    "    if not os.path.isfile(video_path):\n",
    "        continue\n",
    "\n",
    "    container = av.open(video_path)\n",
    "    for idx, frame in enumerate(container.decode(video=0)):\n",
    "        img = np.array(frame.to_image())\n",
    "\n",
    "        if idx == frame_num:\n",
    "            break\n",
    "\n",
    "    batch = img[np.newaxis, ...]\n",
    "    face = fd.crop_and_align(batch, [bbox_[np.newaxis, ...]], [landmark_[np.newaxis, ...]])\n",
    "    face = np.squeeze(face)\n",
    "\n",
    "    img_write_path = os.path.join('./DEBUG/faces',\n",
    "                                str(label_), \n",
    "                                f\"{diautt_}_frame{frame_num}_{'_'.join([str(foo) for foo in bbox_.astype(np.int).tolist()[:4]])}.jpg\")\n",
    "\n",
    "    cv2.imwrite(img_write_path, cv2.cvtColor(face, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import av\n",
    "import random\n",
    "from cltl_face_all.face_alignment import FaceDetection\n",
    "\n",
    "labels = np.load(\"./DEBUG/embeddings-clusters.npy\")\n",
    "embeddings_all = np.load(\"./DEBUG/embeddings-all.npy\")\n",
    "\n",
    "assert len(labels) == len(embeddings_all)"
   ]
  },
  {
   "source": [
    "# I got below after going through all of them\n",
    "\n",
    "# -1 Random faces\n",
    "# 0 Chandler\n",
    "# 1\tRandom faces\n",
    "# 2\tJoey\n",
    "# 3 Rachel\n",
    "# 4 Monica\n",
    "# 5 Phoebe\n",
    "# 6 Ross\n",
    "# 7 Noise\n",
    "\n",
    "to_keep = {actor: [] for actor in ACTORS}\n",
    "\n",
    "label2name = {\n",
    "    0: 'Chandler',\n",
    "    2: 'Joey',\n",
    "    3: 'Rachel',\n",
    "    4: 'Monica',\n",
    "    5: 'Phoebe',\n",
    "    6: 'Ross'}\n",
    "\n",
    "assert len(to_keep) == len(label2name)\n",
    "\n",
    "for l, e in zip(labels, embeddings_all):\n",
    "    if l not in list((label2name).keys()):\n",
    "        continue\n",
    "    to_keep[label2name[l]].append(e)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chandler 6833\nJoey 7505\nMonica 5144\nPhoebe 6134\nRachel 6275\nRoss 8548\n"
     ]
    }
   ],
   "source": [
    "for key, val in to_keep.items():\n",
    "    print(key, len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chandler (512,) 1.0 float32\nJoey (512,) 1.0000001 float32\nMonica (512,) 1.0 float32\nPhoebe (512,) 1.0000001 float32\nRachel (512,) 1.0 float32\nRoss (512,) 1.0 float32\n"
     ]
    }
   ],
   "source": [
    "final_vectors = {}\n",
    "for name, list_of_embs in to_keep.items():\n",
    "    sum_of_vecs = np.sum(list_of_embs, axis=0)\n",
    "    sum_of_vecs = sum_of_vecs / np.linalg.norm(sum_of_vecs)\n",
    "    print(name, sum_of_vecs.shape, np.linalg.norm(sum_of_vecs), sum_of_vecs.dtype)\n",
    "    final_vectors[name] = sum_of_vecs\n",
    "\n",
    "np.save('DEBUG/friends-embeddings.npy', final_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for name, final_vector in final_vectors.items():\n",
    "    os.makedirs(f'DEBUG/friends/{name}', exist_ok=True)\n",
    "    np.save(f'DEBUG/friends/{name}/{name}.npy', final_vector)"
   ]
  }
 ]
}