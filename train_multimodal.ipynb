{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6f0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kvu/erc/libs')\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas\n",
    "import torch\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tvtf\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.device import detach, move_to\n",
    "\n",
    "from utils.getter import get_instance\n",
    "from datasets import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7044e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id = 'cuda:0' \\\n",
    "    if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc7d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ContextAwareDataset(csv_path='/home/kvu/erc/data/raw-audios-wav/train.csv', \n",
    "                              audio_feat_dir='/home/kvu/erc/Datasets/MELD/audio-features/train', \n",
    "                              text_feat_dir='/home/kvu/erc/Datasets/MELD/text-features/train', \n",
    "                              ordered_json_list='/home/kvu/erc/Datasets/MELD/utterance-ordered.json', \n",
    "                              num_utt=8, dataset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e83e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DataLoader(dataset=dataset, batch_size=8, shuffle=False, pin_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21dec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataset_loader))\n",
    "# x = move_to(x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8382018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, text = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af6f3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0218, -0.0711,  0.0420,  ...,  0.0138,  0.0438,  0.0177],\n",
       "        [-0.0052, -0.1040,  0.0576,  ..., -0.0075,  0.0242, -0.0240],\n",
       "        [ 0.0040, -0.0343,  0.0428,  ..., -0.0112, -0.0089,  0.0491]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ee050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6e42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, num_classes, AUDIO_FEAT_DIM=1280, TEXT_FEAT_DIM=1024):\n",
    "        super().__init__()\n",
    "        self.AUDIO_FEAT_DIM = AUDIO_FEAT_DIM\n",
    "        self.TEXT_FEAT_DIM = TEXT_FEAT_DIM\n",
    "        self.mbp = MBP(AUDIO_FEAT_DIM, TEXT_FEAT_DIM)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            # nn.Linear(AUDIO_FEAT_DIM + TEXT_FEAT_DIM, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        self.fc = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        audio_feats, text_feats = inp\n",
    "        batch_size, num_utt, _ = audio_feats.shape\n",
    "        res = torch.zeros(size=(batch_size, num_utt, 1000))\n",
    "        for i in range(batch_size):\n",
    "            audio_feat = audio_feats[i].float()\n",
    "            text_feat = text_feats[i].float()\n",
    "            fused = self.mbp(audio_feat, text_feat)\n",
    "            print(fused.shape)\n",
    "            res[i] = fused\n",
    "        return res\n",
    "\n",
    "class MBP(nn.Module):\n",
    "    \"\"\"\n",
    "        Multi-modal Factorized Bilinear Pooling - https://arxiv.org/pdf/1708.01471.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, AUDIO_FEAT_DIM, TEXT_FEAT_DIM, SUM_POOLING_WINDOW=3, OUTPUT_DIM=1000):\n",
    "        super().__init__()\n",
    "        self.AUDIO_FEAT_DIM = AUDIO_FEAT_DIM\n",
    "        self.TEXT_FEAT_DIM = TEXT_FEAT_DIM\n",
    "        self.SUM_POOLING_WINDOW = SUM_POOLING_WINDOW\n",
    "        self.OUTPUT_DIM = OUTPUT_DIM\n",
    "        self.FUSED_DIM = SUM_POOLING_WINDOW * OUTPUT_DIM\n",
    "\n",
    "        self.audio_linear_projection = nn.Linear(AUDIO_FEAT_DIM, self.FUSED_DIM)\n",
    "        self.text_linear_projection = nn.Linear(TEXT_FEAT_DIM, self.FUSED_DIM)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        x = self.audio_linear_projection(audio)\n",
    "        y = self.text_linear_projection(text)\n",
    "        z = torch.mul(x, y)\n",
    "        z = self.dropout(z)\n",
    "        z = z.view(-1, 1, self.OUTPUT_DIM, self.SUM_POOLING_WINDOW)\n",
    "        z = torch.sum(z, dim=3)\n",
    "        z = torch.squeeze(z)\n",
    "        z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n",
    "        z = F.normalize(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a3f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = FusionModel(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4627f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n",
      "torch.Size([8, 1000])\n"
     ]
    }
   ],
   "source": [
    "res = fusion(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a8b8c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 1000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b825060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0495,  0.0227, -0.0216,  ..., -0.0345, -0.0199,  0.0252],\n",
       "        [-0.0409,  0.0411,  0.0243,  ...,  0.0335, -0.0117,  0.0170],\n",
       "        [ 0.0202,  0.0652,  0.0254,  ...,  0.0331,  0.0056, -0.0263],\n",
       "        ...,\n",
       "        [-0.0132, -0.0082, -0.0351,  ...,  0.0141, -0.0455, -0.0116],\n",
       "        [ 0.0148,  0.0291, -0.0170,  ...,  0.0412, -0.0304, -0.0283],\n",
       "        [ 0.0380,  0.0259,  0.0147,  ...,  0.0206,  0.0020,  0.0341]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:, -1, :] # Pick the last vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbbbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
