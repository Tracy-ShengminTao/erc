{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/repos/erc\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MELD_IEMOCAP', None, 0, 0, 42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = \"./multimodal-datasets/\"\n",
    "model_checkpoint = \"/home/tk/repos/erc/emoberta-large\"\n",
    "DATASET = \"MELD_IEMOCAP\"\n",
    "speaker_mode = None\n",
    "num_past_utterances = 0\n",
    "num_future_utterances = 0\n",
    "SEED = 42\n",
    "\n",
    "DATASET, speaker_mode, num_past_utterances, num_future_utterances, SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from utils import get_num_classes, ErcTextDataset, compute_metrics\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "NUM_CLASSES = get_num_classes(DATASET)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=NUM_CLASSES,\n",
    ")\n",
    "# model.push_to_hub(\"emoberta-large\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 12:39:07.725 INFO utils - _string2tokens: converting utterances into tokens ...\n",
      "2022-03-15 12:39:07.727 INFO utils - _string2tokens: creating input utterance data ... \n",
      "100%|██████████| 1424/1424 [00:06<00:00, 237.23it/s]\n",
      "2022-03-15 12:39:19.499 INFO utils - _create_input: number of truncated utterances: 0\n",
      "2022-03-15 12:39:19.510 INFO utils - _string2tokens: converting utterances into tokens ...\n",
      "2022-03-15 12:39:19.511 INFO utils - _string2tokens: creating input utterance data ... \n",
      "100%|██████████| 159/159 [00:00<00:00, 197.55it/s]\n",
      "2022-03-15 12:39:25.665 INFO utils - _create_input: number of truncated utterances: 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "ds = {}\n",
    "\n",
    "ds[\"train\"] = ErcTextDataset(\n",
    "    DATASET=DATASET,\n",
    "    SPLIT=\"train\",\n",
    "    speaker_mode=speaker_mode,\n",
    "    num_past_utterances=num_past_utterances,\n",
    "    num_future_utterances=num_future_utterances,\n",
    "    model_checkpoint=\"roberta-base\",\n",
    "    ROOT_DIR=ROOT_DIR,\n",
    "    SEED=SEED,\n",
    ")\n",
    "\n",
    "ds[\"val\"] = ErcTextDataset(\n",
    "    DATASET=DATASET,\n",
    "    SPLIT=\"val\",\n",
    "    speaker_mode=speaker_mode,\n",
    "    num_past_utterances=num_past_utterances,\n",
    "    num_future_utterances=num_future_utterances,\n",
    "    model_checkpoint=\"roberta-base\",\n",
    "    ROOT_DIR=ROOT_DIR,\n",
    "    SEED=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "def get_random_sample(ds, tokenizer, idx=None, max_tokens=512):\n",
    "    while True:\n",
    "        if idx is None:\n",
    "            idx_ = np.random.randint(0, len(ds))\n",
    "        else:\n",
    "            idx_ = idx\n",
    "        random_sample = ds[idx_]\n",
    "        input_ids, attention_mask, labelid = (\n",
    "            random_sample[\"input_ids\"],\n",
    "            random_sample[\"attention_mask\"],\n",
    "            random_sample[\"label\"],\n",
    "        )\n",
    "        break\n",
    "    #         if len(input_ids) < max_tokens and labelid != 0:\n",
    "    #             break\n",
    "\n",
    "    decoded = tokenizer.decode(input_ids)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).view(-1, len(input_ids))\n",
    "    attention_mask = torch.tensor(attention_mask).view(-1, len(attention_mask))\n",
    "    labelid = torch.tensor(labelid).view(-1, 1)\n",
    "\n",
    "    return idx_, input_ids, attention_mask, labelid, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2abbc298ea4e049d7652ae1f3a04ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9b22d4b0ec476b8fa3183fd91a1c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 12:39:28.444325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-15 12:39:28.444339: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0 1.0\n",
      "train 1000 0.8056581891628938\n",
      "train 2000 0.8112814137451431\n",
      "train 3000 0.8064255690651801\n",
      "train 4000 0.8084670423628071\n",
      "train 5000 0.8135609028550669\n",
      "train 6000 0.8137845495072008\n",
      "train 7000 0.8106228334211105\n",
      "train 8000 0.8103210140370583\n",
      "train 9000 0.8098046924690303\n",
      "train 10000 0.8121084242925553\n",
      "train 11000 0.8108629168698758\n",
      "train 12000 0.8115574320276991\n",
      "train 13000 0.8118938309023247\n",
      "train 14000 0.8133958936097679\n",
      "train 15000 0.8127758515110489\n",
      "train 16000 0.813287044925905\n",
      "train 17000 0.8133496055833564\n",
      "train 18000 0.8142713641861221\n",
      "train 18890 0.8149141650021035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052279564f3f4305b8cbbe35ef5e0978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 1.0\n",
      "val 1000 0.6440798287108728\n",
      "val 2000 0.6189108382840852\n",
      "val 2345 0.6281057228463983\n"
     ]
    }
   ],
   "source": [
    "for split in tqdm([\"train\", \"val\"]):\n",
    "    truths = []\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(ds[split]))):\n",
    "        idx, input_ids, attention_mask, labelid, decoded = get_random_sample(\n",
    "            ds[split], tokenizer, idx=i\n",
    "        )\n",
    "        outputs = model(\n",
    "            **{\n",
    "                \"input_ids\": input_ids.to(device),\n",
    "                \"attention_mask\": attention_mask.to(device),\n",
    "            },\n",
    "            labels=labelid.to(device),\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        truths.append(labelid.detach().cpu().numpy())\n",
    "        preds.append(torch.softmax(outputs[\"logits\"].detach().cpu(), dim=1).numpy())\n",
    "        f1_weighted = f1_score(\n",
    "            [foo.item() for foo in truths],\n",
    "            [foo.argmax(axis=1).item() for foo in preds],\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(split, i, f1_weighted)\n",
    "    print(split, i, f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, input_ids, attention_mask, labelid, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"adsf\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer(\"adsf\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b3dbf2f99e3477c7bf7d3bb39f34b52463cbb4531bd9ffcdc3498dcac4ea953"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('erc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
