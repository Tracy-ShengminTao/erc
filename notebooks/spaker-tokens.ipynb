{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92d1b0a-d827-495d-9962-8affd01ad67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tae898/repos/erc\n"
     ]
    }
   ],
   "source": [
    "%cd /home/tae898/repos/erc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629ceb3e-9951-4370-bedb-2c0ff9977b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tae898/.virtualenvs/dev-python3.7/lib/python3.7/site-packages/ipykernel_launcher.py:16: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADD_BOU_EOU': True,\n",
       " 'ADD_SPEAKER_TOKENS': True,\n",
       " 'BATCH_SIZE': 4,\n",
       " 'DATASET': 'MELD',\n",
       " 'HP_N_TRIALS': 5,\n",
       " 'HP_ONLY_UPTO': 103,\n",
       " 'NUM_TRAIN_EPOCHS': 5,\n",
       " 'REPLACE_NAMES_IN_UTTERANCES': False,\n",
       " 'SEEDS': [0, 1, 2, 3, 4],\n",
       " 'SPEAKER_SPLITS': ['train', 'val', 'test'],\n",
       " 'WARMUP_RATIO': 0.2,\n",
       " 'WEIGHT_DECAY': 0.01,\n",
       " 'model_checkpoint': 'roberta-large',\n",
       " 'num_future_utterances': 1000,\n",
       " 'num_past_utterances': 1000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = './multimodal-datasets/'\n",
    "model_checkpoint = 'results/MELD/roberta-large/2021-05-22-19-51-21/'\n",
    "# model_checkpoint = 'results/IEMOCAP/roberta-large/2021-05-21-07-58-04/3/checkpoint-4780/'\n",
    "DATASET = model_checkpoint.split('/')[1]\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as stream:\n",
    "        foo = json.load(stream)\n",
    "    return foo\n",
    "\n",
    "def read_yaml(path):\n",
    "    with open(path, 'r') as stream:\n",
    "        foo = yaml.load(stream)\n",
    "    return foo\n",
    "\n",
    "\n",
    "kwargs_path = f\"{'/'.join(model_checkpoint.split('/')[:-3])}/kwargs.yaml\"    \n",
    "kwargs = read_yaml(kwargs_path)\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc09f622-7af3-49ed-a540-f5b29494d1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 14:02:02.488 INFO utils - _load_utterance_ordered: every text file exists fine!\n",
      "2021-05-21 14:02:02.489 INFO utils - _string2tokens: converting utterances into tokens ...\n",
      "2021-05-21 14:02:02.491 INFO utils - _string2tokens: creating input utterance data ... \n",
      "100%|██████████| 1038/1038 [00:47<00:00, 22.07it/s]\n",
      "2021-05-21 14:02:50.429 INFO utils - _create_input: number of truncated utterances: 68\n",
      "2021-05-21 14:02:50.483 INFO utils - _load_utterance_ordered: every text file exists fine!\n",
      "2021-05-21 14:02:50.484 INFO utils - _string2tokens: converting utterances into tokens ...\n",
      "2021-05-21 14:02:50.485 INFO utils - _string2tokens: creating input utterance data ... \n",
      "100%|██████████| 114/114 [00:03<00:00, 29.70it/s]\n",
      "2021-05-21 14:02:55.223 INFO utils - _create_input: number of truncated utterances: 0\n",
      "2021-05-21 14:02:55.308 INFO utils - _load_utterance_ordered: every text file exists fine!\n",
      "2021-05-21 14:02:55.309 INFO utils - _string2tokens: converting utterances into tokens ...\n",
      "2021-05-21 14:02:55.310 INFO utils - _string2tokens: creating input utterance data ... \n",
      "100%|██████████| 280/280 [00:11<00:00, 24.01it/s]\n",
      "2021-05-21 14:03:07.882 INFO utils - _create_input: number of truncated utterances: 33\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from utils import get_num_classes, ErcTextDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_CLASSES = get_num_classes(DATASET)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "    model_checkpoint)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=NUM_CLASSES)\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "SEED = int(model_checkpoint.split('/')[-3])\n",
    "print(SEED)\n",
    "\n",
    "ds_train = ErcTextDataset(DATASET=DATASET, SPLIT='train',\n",
    "                          num_past_utterances=kwargs['num_past_utterances'], num_future_utterances=kwargs['num_future_utterances'],\n",
    "                          model_checkpoint=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer\",\n",
    "                          ADD_BOU_EOU=kwargs['ADD_BOU_EOU'], ADD_SPEAKER_TOKENS=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer/added_tokens.json\",\n",
    "                          REPLACE_NAMES_IN_UTTERANCES=kwargs['REPLACE_NAMES_IN_UTTERANCES'],\n",
    "                          ROOT_DIR=ROOT_DIR, SEED=SEED)\n",
    "\n",
    "ds_val = ErcTextDataset(DATASET=DATASET, SPLIT='val',\n",
    "                          num_past_utterances=kwargs['num_past_utterances'], num_future_utterances=kwargs['num_future_utterances'],\n",
    "                          model_checkpoint=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer\",\n",
    "                          ADD_BOU_EOU=kwargs['ADD_BOU_EOU'], ADD_SPEAKER_TOKENS=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer/added_tokens.json\",\n",
    "                          REPLACE_NAMES_IN_UTTERANCES=kwargs['REPLACE_NAMES_IN_UTTERANCES'],\n",
    "                          ROOT_DIR=ROOT_DIR, SEED=SEED)\n",
    "\n",
    "ds_test = ErcTextDataset(DATASET=DATASET, SPLIT='test',\n",
    "                          num_past_utterances=kwargs['num_past_utterances'], num_future_utterances=kwargs['num_future_utterances'],\n",
    "                          model_checkpoint=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer\",\n",
    "                          ADD_BOU_EOU=kwargs['ADD_BOU_EOU'], ADD_SPEAKER_TOKENS=f\"{'/'.join(model_checkpoint.split('/')[:-3])}/tokenizer/added_tokens.json\",\n",
    "                          REPLACE_NAMES_IN_UTTERANCES=kwargs['REPLACE_NAMES_IN_UTTERANCES'],\n",
    "                          ROOT_DIR=ROOT_DIR, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8b885f-0206-40c8-a909-b4e65db70270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1033\n",
      "\n",
      "diaid: dia87 \n",
      "uttid: dia87_utt0\n",
      "\n",
      "[0, 2, 2, 50497, 7516, 734, 15983, 6, 2051, 4, 2, 2, 50265, 50497, 1708, 734, 364, 13492, 734, 50266, 50265, 50497, 100, 95, 33, 65, 864, 13, 47, 6, 1437, 1437, 1437, 364, 13492, 734, 50266, 50265, 50497, 1779, 52, 4205, 197, 52, 1656, 6, 50, 422, 6, 50, 3349, 2389, 6, 1437, 1437, 1437, 50, 24808, 734, 50266, 50265, 50301, 22174, 24, 6, 912, 24, 328, 91, 1431, 2635, 53, 37, 429, 32734, 1769, 328, 5148, 6, 1437, 1437, 1437, 905, 18, 213, 12846, 50266, 2]\n",
      "\n",
      "<s></s></s><Ross>Oh... ok, fine.</s></s><u><Ross>But... ehm...</u><u><Ross>I just have one question for you,    ehm...</u><u><Ross>When we exit should we walk, or run, or prance,    or stroll...</u><u><Charlie>Stop it, stop it! He talks slow but he might pee fast! Ok,    let's go!!</u></s>\n",
      "\n",
      "(neutral) <Ross>Oh... ok, fine.\n",
      "\n",
      "'dia87_utt0, (neutral) <Ross>Oh... ok, fine.'\n",
      "'dia87_utt1, (neutral) <Ross>But... ehm...'\n",
      "'dia87_utt2, (neutral) <Ross>I just have one question for you,    ehm...'\n",
      "('dia87_utt3, (neutral) <Ross>When we exit should we walk, or run, or '\n",
      " 'prance,    or stroll...')\n",
      "('dia87_utt4, (joy) <Charlie>Stop it, stop it! He talks slow but he might pee '\n",
      " \"fast! Ok,    let's go!!\")\n"
     ]
    }
   ],
   "source": [
    "SPLIT = 'val'\n",
    "if SPLIT == 'train':\n",
    "    ds = ds_train\n",
    "elif SPLIT == 'val':\n",
    "    ds = ds_val\n",
    "else:\n",
    "    ds = ds_test\n",
    "\n",
    "idx = np.random.randint(0, len(ds))\n",
    "uttid = ds.uttids[idx]\n",
    "diaid = None\n",
    "\n",
    "for diaid_, uttids_ in ds.utterance_ordered.items():\n",
    "    for uttid_ in uttids_:\n",
    "        if uttid_ == uttid:\n",
    "            diaid = diaid_    \n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "utts = {foo.split('/')[-1].split('.json')[0]: f\"({read_json(foo)['Emotion']}) \" + f\"<{read_json(foo)['Speaker']}>\" + read_json(foo)['Utterance'] for foo in glob(f'./multimodal-datasets/{DATASET}/raw-texts/{SPLIT}/{diaid}_*.json')}\n",
    "# utts_keys = [foo[0] for foo in sorted([(foo, int(foo.split('utt')[1])) for foo in list(utts.keys())], key=lambda x:x[1])]\n",
    "utts = {key: utts[key] for key in ds.utterance_ordered[diaid]}\n",
    "utt = utts[uttid]\n",
    "input_ids = ds[idx]['input_ids']\n",
    "\n",
    "print(f'idx: {idx}')\n",
    "print()\n",
    "print(f\"diaid: {diaid} \\nuttid: {uttid}\")\n",
    "print()\n",
    "print(input_ids)\n",
    "print()\n",
    "print(tokenizer.decode(input_ids))\n",
    "print()\n",
    "print(utt)\n",
    "print()\n",
    "for key, val in utts.items():\n",
    "    pprint(f\"{key}, {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45abd9e-a2c8-471a-be02-f3c3af299156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(neutral) <Ross>Oh... ok, fine.\n",
      "{'input_ids': [0, 1640, 12516, 43, 1437, 50497, 7516, 734, 15983, 6, 2051, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "(neutral) <<Ross>>Oh... ok, fine.\n",
      "{'input_ids': [0, 1640, 12516, 43, 28696, 50497, 15698, 7516, 734, 15983, 6, 2051, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# utt = 'I am a stupid fuck, Im Ross'\n",
    "print(utt)\n",
    "print(tokenizer(utt))\n",
    "\n",
    "for token, token_id in ds.added_tokens.items():\n",
    "    if token in ['<u>', '</u>']:\n",
    "        continue\n",
    "    token_stripped = token.split('<')[-1].split('>')[0]\n",
    "    \n",
    "    utt = utt.replace(token_stripped, token)\n",
    "    \n",
    "print()\n",
    "\n",
    "print(utt)\n",
    "print(tokenizer(utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba4161-6cbe-463d-addf-d996b4c94776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
