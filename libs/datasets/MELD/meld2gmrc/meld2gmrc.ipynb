{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitgmrcf81cfdfab8954b429b0a01dc90383b31",
   "display_name": "Python 3.7.9 64-bit ('gmrc')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Set the paths\n",
    "\n",
    "Download the visual features, and raw videos if you haven't yet.\n",
    "And then set the paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download visaul features\n",
    "# !gdown --id 1jS2ufbIovxg8umkZM5UKzsvtSp4UJJyt\n",
    "\n",
    "VISUAL_FEATURES_DIR = {}\n",
    "VISUAL_FEATURES_DIR['train'] = \"/home/tk/datasets/MELD/visual-features/MELD-visual-features/train/\"\n",
    "VISUAL_FEATURES_DIR['dev'] = \"/home/tk/datasets/MELD/visual-features/MELD-visual-features/dev/\"\n",
    "VISUAL_FEATURES_DIR['test'] = \"/home/tk/datasets/MELD/visual-features/MELD-visual-features/test/\"\n",
    "\n",
    "# download the raw videos\n",
    "# !wget http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz\n",
    "VIDS_DIR = {}\n",
    "VIDS_DIR['train'] = \"/home/tk/datasets/MELD/MELD.Raw/train/train_splits/\"\n",
    "VIDS_DIR['dev'] = \"/home/tk/datasets/MELD/MELD.Raw/dev/dev_splits_complete/\"\n",
    "VIDS_DIR['test'] = \"/home/tk/datasets/MELD/MELD.Raw/test/output_repeated_splits_test\"\n",
    "\n",
    "# This comes with VISUAL_FEATURES when you downloaded them.\n",
    "ANNOTATION_PATH = \"/home/tk/datasets/MELD/visual-features/MELD-visual-features/datasets.json\"\n",
    "\n",
    "THRESHOLDS = {'face': 0.9, 'angle': 1.15}\n",
    "\n",
    "SAVE_AT = './DEBUG'\n",
    "\n",
    "# in seconds\n",
    "IMAGE_INTERVAL = 0.5"
   ]
  },
  {
   "source": [
    "## Read pre-computed visual features and annotations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import av\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "with open(ANNOTATION_PATH, 'r') as stream:\n",
    "    datasets = json.load(stream)\n",
    "\n",
    "visual_features = {DATASET: glob(os.path.join(VISUAL_FEATURES_DIR[DATASET], '*.npy'))\n",
    "                   for DATASET in ['train', 'dev', 'test']}\n",
    "\n",
    "visual_features = {DATASET: {os.path.basename(vf).split('.npy')[0] : vf \n",
    "                   for vf in tqdm(visual_features[DATASET])}\n",
    "                   for DATASET in tqdm(['train', 'dev', 'test'])}\n",
    "\n",
    "with open('friends-time/friends-time.pkl', 'rb') as stream:\n",
    "    friends_time = pickle.load(stream)"
   ]
  },
  {
   "source": [
    "## Define helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import csv\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "MAIN_ACTORS = {}\n",
    "for path in glob('../main-actors/*.npy'):\n",
    "    name = path.split('/')[-1].split('.npy')[0]\n",
    "    MAIN_ACTORS[name] = np.load(path)\n",
    "\n",
    "def calc_angle_distance(emb1, emb2):\n",
    "    \"\"\"Calculate the angle (radian) distance between the embeddings.\"\"\"\n",
    "    return np.arccos(np.clip((emb1 @ emb2.T), -1, 1))\n",
    "\n",
    "def get_unique_dias(list_of_diautts):\n",
    "    return sorted(list(set([diautt.split('_')[0] for diautt in list_of_diautts])))\n",
    "\n",
    "def get_time_unix_ms(time_string, season, episode):\n",
    "    hours, minutes, seconds = time_string.split(':')\n",
    "    seconds, milliseconds = seconds.split(',')\n",
    "    hours, minutes, seconds, milliseconds = int(hours), int(minutes), int(seconds), int(milliseconds)\n",
    "    time_datetime = friends_time[season][episode] + timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "    time_unix = time.mktime(time_datetime.timetuple())\n",
    "    time_unix_ms = int(time_unix*1000 + milliseconds)    \n",
    "\n",
    "    return time_unix_ms\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "class Utterance():\n",
    "\n",
    "    def __init__(self, diautt, annotation, visual_feature_path, video_path, audio_path, image_interval):\n",
    "        self.diautt = diautt\n",
    "        self.srno = annotation['SrNo']\n",
    "        self.utterance = annotation['Utterance']\n",
    "        self.speaker = annotation['Speaker']\n",
    "        self.emotion = annotation['Emotion']\n",
    "        self.sentiment = annotation['Sentiment']\n",
    "        self.dialogue_id = annotation['Dialogue_ID']\n",
    "        self.utterance_id = annotation['Utterance_ID']\n",
    "        self.season = annotation['Season']\n",
    "        self.episode = annotation['Episode']\n",
    "        self.starttime = get_time_unix_ms(annotation['StartTime'], self.season, self.episode)\n",
    "        self.endtime = get_time_unix_ms(annotation['EndTime'], self.season, self.episode)\n",
    "\n",
    "        self.visual_features = np.load(visual_feature_path, allow_pickle=True).item()\n",
    "        self.video_path = video_path\n",
    "        assert os.path.isfile(self.video_path)\n",
    "        self.audio_path = audio_path\n",
    "        self.image_interval = image_interval\n",
    "\n",
    "    def write_to_audio(self):\n",
    "        # !ffmpeg -i $self.video_path -q:a 0 -map a $self.audio_path -loglevel quiet\n",
    "        !ffmpeg -i $self.video_path -q:a 0 -map a $self.audio_path > /dev/null 2>&1 < /dev/null\n",
    "\n",
    "    def run_on_video(self, dia_dir):\n",
    "        container = av.open(self.video_path)\n",
    "        fps = float(container.streams.video[0].average_rate)\n",
    "        spf = 1/fps \n",
    "        mspf = round(spf * 1000)\n",
    "\n",
    "        self.video_info = []\n",
    "        for idx, frame in enumerate(container.decode(video=0)):\n",
    "            if idx % round(self.image_interval * fps) != 0:\n",
    "                continue\n",
    "\n",
    "            numpy_RGB = np.array(frame.to_image())\n",
    "            numpy_BGR = cv2.cvtColor(numpy_RGB, cv2.COLOR_RGB2BGR)\n",
    "            frame_time = idx*mspf + self.starttime\n",
    "            img_path = os.path.join(dia_dir, 'image',\n",
    "                                    self.diautt + f'_frame{str(idx)}_{str(frame_time)}.jpg')\n",
    "            cv2.imwrite(img_path, numpy_BGR)\n",
    "\n",
    "            features = self.visual_features[idx]\n",
    "\n",
    "            if not features:\n",
    "                continue\n",
    "\n",
    "            frame_info = {}\n",
    "            frame_info['files'] = [os.path.join('image', os.path.basename(img_path))]\n",
    "            container_id = str(uuid.uuid4())\n",
    "            frame_info['id'] = container_id\n",
    "            frame_info['mentions'] = []\n",
    "\n",
    "            frame_info['modality'] = 'image'\n",
    "            frame_info['ruler'] = {'bounds': [0, 0, numpy_BGR.shape[1], numpy_BGR.shape[0]],\n",
    "                                       'container_id': container_id,\n",
    "                                       'type': 'MultiIndex'}\n",
    "            frame_info['time'] = {'container_id': container_id,\n",
    "                                      'start': frame_time,\n",
    "                                        'end': frame_time + mspf,\n",
    "                                        'type': 'TemporalRuler'}\n",
    "            frame_info['type'] = 'ImageSignal'\n",
    "\n",
    "\n",
    "            for k, feat in enumerate(features):\n",
    "                age = round(float(feat['age']), 3)\n",
    "                gender = round(float(feat['gender']), 3)\n",
    "                bbox = feat['bbox']\n",
    "                bbox, faceprob = [int(round(bb)) for bb in bbox[:4]], float(bbox[-1])\n",
    "                faceprob = round(faceprob, 3)\n",
    "                embedding = feat['embedding']\n",
    "                landmark = feat['landmark']\n",
    "\n",
    "                annotations = []\n",
    "\n",
    "                embedding.reshape(1, 512)\n",
    "                dists = {key: calc_angle_distance(embedding, val) for key, val \\\n",
    "                            in MAIN_ACTORS.items()}\n",
    "                face_candidate = min(dists, key=dists.get)\n",
    "\n",
    "                if dists[face_candidate] > THRESHOLDS['angle']:\n",
    "                    face_candidate = 'unidentified'\n",
    "\n",
    "                if face_candidate == self.speaker:\n",
    "                    annotations.append({'source': 'human',\n",
    "                                        'timestamp': frame_time,\n",
    "                                        'type': 'emotion',\n",
    "                                        'value': self.emotion.upper()})\n",
    "\n",
    "                # annotations.append({'source': 'machine',\n",
    "                #                     'timestamp': frame_time,\n",
    "                #                     'type': 'display',\n",
    "                #                     'value': face_candidate})\n",
    "\n",
    "                annotations.append({'source': 'machine',\n",
    "                                    'timestamp': frame_time,\n",
    "                                    'type': 'person',\n",
    "                                    'value': {'name': face_candidate,\n",
    "                                              'age': age,\n",
    "                                              'gender': gender,\n",
    "                                              'faceprob': faceprob}})\n",
    "                \n",
    "                mention_id = str(uuid.uuid4())\n",
    "                segment = [{'bounds': bbox,\n",
    "                            'container_id': container_id,\n",
    "                            'type': 'MultiIndex'}]\n",
    "                frame_info['mentions'].append({'annotations': annotations,\n",
    "                                                    'id': mention_id,\n",
    "                                                    'segment': segment})\n",
    "            self.video_info.append(frame_info)"
   ]
  },
  {
   "source": [
    "## Run on the videos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_thread(DATASET, dia_diautts):\n",
    "    for dia, diautts in tqdm(dia_diautts.items()):\n",
    "        shutil.rmtree(os.path.join(SAVE_AT, DATASET, dia), ignore_errors=True)\n",
    "        os.makedirs(os.path.join(SAVE_AT, DATASET, dia, 'image'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(SAVE_AT, DATASET, dia, 'text'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(SAVE_AT, DATASET, dia, 'audio'), exist_ok=True)\n",
    "\n",
    "        chat_gmrc = []\n",
    "        image_gmrc = []\n",
    "        for diautt in diautts:\n",
    "            try:\n",
    "                u = Utterance(diautt=diautt,\n",
    "                                annotation=datasets[DATASET][diautt],\n",
    "                                visual_feature_path=visual_features[DATASET][diautt],\n",
    "                                video_path=os.path.join(VIDS_DIR[DATASET], diautt) + '.mp4',\n",
    "                                audio_path=os.path.join(SAVE_AT, DATASET, dia, 'audio', diautt + '.mp3'),\n",
    "                                image_interval=IMAGE_INTERVAL)\n",
    "            except Exception as e:\n",
    "                print(f\"cannot instantiate an utterance object: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            chat_gmrc.append([u.speaker, u.utterance, u.starttime, u.emotion])\n",
    "\n",
    "\n",
    "            try:\n",
    "                with time_limit(15):\n",
    "                    u.write_to_audio()\n",
    "            except TimeoutException as e:\n",
    "                print(f\"Timed out!: {e}\")\n",
    "\n",
    "            u.run_on_video(dia_dir=os.path.join(SAVE_AT, DATASET, dia))\n",
    "            for video_info in u.video_info:\n",
    "                image_gmrc.append(video_info)\n",
    "\n",
    "        with open(os.path.join(SAVE_AT, DATASET, dia, 'text', f'{dia}.csv'), 'w') as stream:\n",
    "            stream.write('speaker,utterance,time,emotion\\n')\n",
    "\n",
    "            for line in chat_gmrc:\n",
    "                speaker, utterance, starttime, emotion= line\n",
    "                stream.write(speaker)\n",
    "                stream.write(',')\n",
    "                stream.write(f\"\\\"{utterance}\\\"\")\n",
    "                stream.write(',')\n",
    "                stream.write(str(starttime))\n",
    "                stream.write(',')\n",
    "                stream.write(emotion)\n",
    "                stream.write('\\n')    \n",
    "\n",
    "        with open(os.path.join(SAVE_AT, DATASET, dia, 'image.json'), 'w') as stream:\n",
    "            json.dump(image_gmrc, stream)\n",
    "\n",
    "def batch_a_dict(dict_to_batch, num_batches):\n",
    "    batch_size = len(dict_to_batch) // num_batches\n",
    "    keys = (list(dict_to_batch.keys()))\n",
    "    random.shuffle(keys)\n",
    "\n",
    "    batches = [keys[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    batches[-1] = keys[batch_size*(num_batches-1):]\n",
    "\n",
    "    batches = [{key:dict_to_batch[key] for key in batch} for batch in batches]\n",
    "\n",
    "    return batches\n",
    "\n",
    "for DATASET in tqdm(['train', 'dev', 'test']):\n",
    "    dia_diautts = list(datasets[DATASET].keys())\n",
    "    dia_diautts = {dia: [diautt for diautt in dia_diautts if dia + '_' in diautt] \n",
    "                    for dia in get_unique_dias(dia_diautts)}\n",
    "\n",
    "    # TODO: fix the below line.\n",
    "    # dia_diautts = batch_a_dict(dia_diautts, NUM_CORES)\n",
    "    \n",
    "    run_thread(DATASET, dia_diautts)"
   ]
  }
 ]
}