{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../libs/\")\n",
    "from utils.getter import get_instance\n",
    "from utils.random_seed import set_seed\n",
    "import yaml\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "config = yaml.load(open('/home/tkm290/repos/erc/configs/timesformer.yaml', \"r\"), Loader=yaml.Loader)\n",
    "\n",
    "\n",
    "invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding common utterances in video, face, and audio ...\n",
      "In total there are 9222 utterances in common, across the two modalities\n",
      "finding common utterances in video, face, and audio ...\n",
      "In total there are 1316 utterances in common, across the two modalities\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "train_dataset = get_instance(config[\"dataset\"][\"train\"])\n",
    "train_dataloader = get_instance(\n",
    "    config[\"dataset\"][\"train\"][\"loader\"], dataset=train_dataset\n",
    ")\n",
    "\n",
    "set_seed()\n",
    "val_dataset = get_instance(config[\"dataset\"][\"val\"])\n",
    "val_dataloader = get_instance(\n",
    "    config[\"dataset\"][\"val\"][\"loader\"], dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, labels = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8a78bed90c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "torch.stack(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    im.set_data(vid[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(vid[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "vid, lbl = train_dataset[1]\n",
    "vid = [invTrans(frame) for frame in vid]\n",
    "vid = [(frame.numpy()*255).astype(np.uint8) for frame in vid]\n",
    "vid = [np.transpose(frame, (1,2,0)) for frame in vid]\n",
    "vid = np.array(vid)\n",
    "# np array with shape (frames, height, width, channels)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "im = plt.imshow(vid[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=vid.shape[0],\n",
    "                               interval=500)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
